%!TEX root = ./template-skripsi.tex
%-------------------------------------------------------------------------------
%                            	BAB IV
%               		KESIMPULAN DAN SARAN
%-------------------------------------------------------------------------------

\chapter{HASIL DAN PEMBAHASAN}

\section{\textit{Training Strong Classifier}}
\subsection{\textit{Input} Gambar dan \textit{labeling}}

	\begin{figure}[H]
		\centering{}
		\includegraphics[width=0.6\textwidth]{gambar/training\_data}
		\caption{Gambar-gambar yang akan dipakai untuk training}
	\end{figure}

	Langkah pertama dalam melatih \textit{classifier} adalah dengan memasukan 
	\textit{dataset} untuk latihan berserta label-labelnya. Gambar pertama dimasukan 
	ke dalam folder sesuai dengan kelasnya. Dalam situasi ini ada empat folder yaitu, 
	untuk kelas satu: abudefduf, untuk kelas dua: amphiprion, untuk kelas tiga: chaetodon 
	dan terakhir untuk kelas nol: negative\_examples. Gambar-gambar tersebut lalu akan dibaca 
	menggunakan \textit{library} CV2 yang bertugas juga untuk mengubah gambar menjadi 
	\textit{greyscale}. Berikut adalah \emph{source code} \texttt{load\_images()}
	yang digunakan untuk membaca set gambar latihan:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def load_images(directory):
				images=[]
				labels=[]
				for filename in os.listdir(directory):
					if filename.endswith(".png"):
						image_path = os.path.join(directory, filename)
						image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
						image = cv2.resize(image, (350, 200))
						images.append(image)
						labels.append(get_label(directory))
				return np.array(images), np.array(labels)
		\end{lstlisting}
		\caption{\emph{source code:} \textit{read} gambar, labelisasi, pengubahan ke \textit{greyscale}, 
		dan memastikan ukuran gambar 350 x 200 piksel}
		\label{code:pre-processing gambar}
	\end{figure}

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def get_label(directory):
				# add more to add more class
				if directory == "fish_dataset\\abudefduf": return 1
				if directory == "fish_dataset\\amphiprion": return 2
				if directory == "fish_dataset\\chaetodon": return 3
				else: return 0
		\end{lstlisting}
		\caption{\emph{source code:} labelisasi gambar sesuai dengan foldernya}
		\label{code:labeling}
	\end{figure}

	Untuk memanggil kedua fungsi \texttt{load\_images} dan \texttt{get\_label} diperlukan sebuah fungsi lainnya 
	yang berfungsi juga untuk menggabungkan semua gambar dan label menjadi dua buah \emph{array} 
	\texttt{images} dan \texttt{labels}. Kedua \emph{array} ini nantinya akan menjadi \textit{dataset} utama 
	dalam proses pelatihan. Berikut \emph{source code} dari \texttt{combine\_dataset} yang 
	digunakan untuk membuat \textit{array} \texttt{images} dan \texttt{labels}:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def combine_dataset():
				# load datasets from directories
				# add class to get_label first or the class will be considered a negative example
				abudefduf_images, abudefduf_labels = load_images("fish_dataset\\abudefduf")
				amphiprion_images, amphiprion_labels = load_images("fish_dataset\\amphiprion")
				chaetodon_images, chaetodon_labels = load_images("fish_dataset\\chaetodon")
				negatives_images, negatives_labels = load_images("fish_dataset\\negative_examples")

				# combining into a single dataset
				images = np.concatenate((abudefduf_images, amphiprion_images, chaetodon_images, negatives_images), axis = 0)
				labels = np.concatenate((abudefduf_labels, amphiprion_labels, chaetodon_labels, negatives_labels), axis = 0)

				return images, labels
		\end{lstlisting}
		\caption{\emph{source code:} \textit{load} gambar-gambar dari folder yang bersangkutan dan 
		menggabungkannya}
		\label{code:conc dataset}
	\end{figure}

\subsection{\textit{Generate Haar-like Features}}
	Untuk melakukan \emph{generate feature} sebuah fungsi bernama \texttt{generate\_features()} dipanggil 
	dengan parameter lebar dan tinggi dari \emph{sub-window} yang akan digunakan. Fungsi ini akan 
	menghasilkan kurang lebih sekitar 520000 fitur berbeda untuk \emph{sub-window} berukuran 
	50 x 50 piksel. Berikut \emph{source code}-nya:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def generate_features(image_height, image_width):
				features = []
				features_list = ["Two Horizontal", "Two Vertical", "Four Diagonal", "Right Triangular", "Left Triangular", "Three Horizontal", "Three Vertical"]
				for i in features_list:
					match i:
						case "Two Horizontal" | "Two Vertical" | "Four Diagonal" | "Right Triangular" | "Left Triangular":
							feature_height = 4
							feature_width = 4
						case "Three Horizontal":
							feature_height = 4
							feature_width = 6
						case "Three Vertical":
							feature_height = 6
							feature_width = 4
					for w in range (feature_width, image_width+1, feature_width):
						for h in range (feature_height, image_height+1, feature_height):
							for x in range (0, image_width - w):
								for y in range (0, image_height - h):
									feature = (i, x, y, w, h)
									features.append(feature)
    			return features
		\end{lstlisting}
		\caption{\emph{source code:} sebuah data dalam \textit{features} memiliki semua informasi yang diperlukan 
		untuk melakukan perhitungan nilai sebuah fitur. Tipe-tipe fitur akan menentukan rumus perhitungan fitur tersebut}
		\label{code:generate features}
	\end{figure}

\subsection{\textit{Calculating all features of all images}}
	Untuk mempermudah proses pembuatan \emph{Decision tree} nantinya, semua fitur 
	pada semua \textit{sub-window}, pada semua gambar akan dihitung dan dimasukan 
	kedalam sebuah dokumen .CSV mengunakan \textit{library} Pandas. Fungsi yang dipakai untuk memulai proses ini adalah 
	fungsi \texttt{write\_csv()} pada Utilities.py. Berikut \emph{source code}-nya:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def write_csv(images, labels, features, csv_name):
				print("starting write_csv")
				for window_num in range(3):
					temp_window_values = np.zeros((len(images), len(features)), dtype=object)
					image_ids = np.arange(len(images))

					for i in range(len(images)):
						new_data = Dataset(images[i], labels[i], features)
						if window_num == 0:
							temp_window_values[i] = new_data.window_1_features
						elif window_num == 1:
							temp_window_values[i] = new_data.window_2_features
						elif window_num == 2:
							temp_window_values[i] = new_data.window_3_features

					window_feature = {'image_ids': image_ids}
					for i in range(len(features)):
						column_name = f'win_{window_num + 1}_feature_{i}'
						window_feature[column_name] = temp_window_values[:, i]

					directory = f"Data/{csv_name}_window_{window_num}.csv"

					window_feature = pd.DataFrame(window_feature)
					window_feature.to_csv(directory, index=False)
				print("csv write complete!")
		\end{lstlisting}
		\caption{\emph{source code:} \texttt{write\_csv()} mengambil semua gambar, 
		label, fitur dan mengkalkulasi semua fitur untuk ketiga \textit{sub-window}}
		\label{code:calculate all features}
	\end{figure}

	Fungsi ini menghasilkan sebuah dokumen .csv untuk setiap \textit{sub-window} dengan
	baris mengikuti jumlah gambar didalam \textit{dataset} dan kolom mengikuti jumlah fitur yang ada. 
	Maka dari itu untuk \emph{dataset} 80 gambar akan dihasilkan sebuah tabel .csv dengan 
	bentuk 80 x 520.000.

	Fungsi ini berjalan cukup lama dengan jumlah gambar 80 buah. Penulis menghitung rata-rata 
	waktu yang diperlukan bagi fungsi untuk membuat sebuah dokumen .csv adalah 45 menit minimum. 
	Dan proses ini dilakukan sampai tiga kali agar dapat menciptakan tiga dokumen .csv untuk ketiga 
	\textit{sub-window}. Dokumen .csv ini nantinya akan diperlukan untuk proses pembuatan \emph{decision tree}.

	\texttt{Dataset} adalah sebuah \textit{class} yang digunakan untuk menyimpan seluruh nilai fitur 
	dari sebuah gambar sebelum dimasukan kedalam dokumen .csv .  
	Berikut adalah bentuk \textit{class} \texttt{Dataset} berserta 
	fungsi bawaannya \texttt{Find\_Feature\_Value}:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			class Dataset:
				def __init__(self, image, label, feature_list):
					self.image = image
					self.label = label
					self.window_1_features = self.Find_Feature_Value(image, feature_list, self.class_Window_offset_1[label][0], self.class_Window_offset_1[label][1])
					self.window_2_features = self.Find_Feature_Value(image, feature_list, self.class_Window_offset_2[label][0], self.class_Window_offset_2[label][1])
					self.window_3_features = self.Find_Feature_Value(image, feature_list, self.class_Window_offset_3[label][0], self.class_Window_offset_3[label][1])

				def Find_Feature_Value(self, image, feature_list, x_offset, y_offset):
					features = np.zeros(len(feature_list), dtype=object)
					for i in range(len(feature_list)):
						feature_type, x, y, width, height = feature_list[i]
						x += x_offset
						y += y_offset
						updated_feature = (feature_type, x, y, width, height)
						data_features = compute_feature_with_matrix(image, 0, updated_feature)
						features[i] = data_features
					return features
				
		\end{lstlisting}
		\caption{\emph{source code:} \textit{class Dataset}}
		\label{code:Dataset class}
	\end{figure}

	\texttt{Dataset}, untuk melakukan perhitungan fitur, digunakan \textit{Offset} 
	menspesifikasi lokasi mulut, sirip dan ekor ikan pada gambar. Hal ini dilakukan 
	karena masing-masing kelas ikan semuanya memiliki 
	mulut, sirip dan ekor dengan lokasi yang berbeda satu dengan yang lainnya. Contohnya: Mulut dari genus ikan Chaetodon 
	agak lebih kebawah dibandingkan kedua kelas ikan lainnya. Selain itu juga untuk setiap bagian 
	ikan yang akan diklasifikasi akan dicari lokasi yang paling terlihat unik daripada kelas-kelas lainnya. Contohnya 
	pada kelas genus ikan Abudefduf diambil bagian ekor yang memberntuk huruf "V" 
	sementara pada kelas genus Amphiprion bagian kelas yang dipelajari adalah 
	bagian melengkung bagian atas diantara ekor dan badan. Berikut adalah 
	\textit{offset} yang penulis gunakan untuk pelatihan:

	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			class_Window_offset_1 = [
				# order according to label's order in LoadImages
				# for searching mouth features
				(0, 0),
				(0, 88),
				(0, 73),
				(15, 100)
			]

			class_Window_offset_2 = [
				# order according to label's order in LoadImages
				# for searching fin features
				(0, 0),
				(116, 0),
				(153, 9),
				(116, 0)
    
			]

			class_Window_offset_3 = [
				# order according to label's order in LoadImages
				# for searching tail feature
				(0, 0),
				(280, 89),
				(237, 47),
				(277, 80)
    ]
		\end{lstlisting}
		\caption{\emph{source code:} \textit{offset} ini di-inisalisasi untuk setiap kelas Dataset 
		sehingga bisa diakses langsung oleh fungsi \texttt{Find\_Feature\_Value}}
		\label{code:Training sub-window offset}
	\end{figure}

	Untuk perhitungan nilai fitur dari gambar digunakan fungsi \texttt{compute\_feature\_with\_matrix()}. 
	Pertama data fitur diubah dengan menambah lokasi x dan y dari fitur dengan \texttt{x\_offset}, dan  \texttt{y\_offset} 
	lalu fitur akan mengembalikan sebuah nilai float dari hasil perhitungan tersebut. Berikut \emph{source code} 
	dari \texttt{compute\_feature\_with\_matrix}:
	
	\begin{figure}[H]
		\begin{lstlisting}[language=Python, basicstyle=\tiny]
			def compute_feature_with_matrix(image, feature):
				feature_type, x, y, width, height = feature
				# +1 due to slicing paramter = start at:stop before
				match feature_type:
					case "Two Horizontal":
						white = np.sum(image[y:y + height + 1, x:x + int(width/2) + 1])
						black = np.sum(image[y:y + height + 1, x + int(width/2):x + width + 1])
					case "Two Vertical":
						white = np.sum(image[y:y + int(height/2) + 1, x:x + width+1])
						black = np.sum(image[y + int(height/2):y + height + 1, x:x + width+1])
					case "Three Horizontal":
						white = np.sum(image[y: y + height + 1, x:x + int(width/3) + 1]) + np.sum(image[y: y + height + 1, x + int(width*2/3):x + width + 1])
						black = np.sum(image[y: y + height + 1, x + int(width/3):x + int(width*2/3) + 1])
					case "Three Vertical":
						white = np.sum(image[y:y + int(height/3) + 1, x:x + width + 1]) + np.sum(image[y + int(height*2/3):y + height + 1, x: x + width + 1])
						black = np.sum(image[y + int(height/3):y + int(height*2/3) + 1, x:x + width + 1])
					case "Four Diagonal":
						white = np.sum(image[y:y + int(height/2) + 1, x + int(width/2): x + width + 1]) + np.sum(image[y + int(height/2):y + height + 1, x: x + int(width/2) + 1])
						black = np.sum(image[y:y + int(height/2) + 1, x:x + int(width/2) + 1]) + np.sum(image[y + int(height/2): y + height + 1, x + int(width/2):x + width + 1])
					case "Right Triangular":
						matrix = image[y:y + height + 1, x:x + width + 1]
						white = np.sum(np.tril(matrix))
						black = np.sum(np.triu(matrix))
					case "Left Triangular":
						matrix = np.rot90(image[y:y + height + 1, x:x + width + 1], k=3)
						white = np.sum(np.tril(matrix))
						black = np.sum(np.triu(matrix))
				return int(white) - int(black)
		\end{lstlisting}
		\caption{\emph{source code:} \textit{offset} ini di-inisalisasi untuk setiap kelas Dataset 
		sehingga bisa diakses langsung oleh fungsi \texttt{Find\_Feature\_Value()}}
		\label{code: feature calculation}
	\end{figure}

	\subsection{\textit{Create Decision Tree for each Feature}}
		Setelah semua nilai fitur sudah dihitung dan dimasukan kedalam dokumen .csv, 
		selanjutnya bisa dimulai proses pembuatan \emph{decision tree} atau \emph{weak classifier}. 
		Proses ini dimulai dengan pertama membagi contoh menjadi tiga kelompok, yaitu data \emph{training}, 
		data \emph{testing} dan data \emph{validation}. Hal ini dilakukan dengan menggunakan fungsi 
		\texttt{split\_data} dibantu dengan fungsi \texttt{train\_test\_split()} dari \textit{library} sklearn:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
				def split_data(features, csv_name, labels):
					data = DecisionTree.get_data(features, csv_name)
					labels_df = pd.DataFrame({'Label' : labels})
					data = pd.concat([data, labels_df], axis=1)

					X = data.iloc[:, :-1].values 
					Y = data.iloc[:, -1].values.reshape(-1, 1)

					X_temp, X_train, Y_temp, Y_train = train_test_split(X, Y, test_size=0.3, random_state=42)
					X_valid, X_test, Y_valid, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

					print(type(X_train))
					splits = [X_train, Y_train, X_test, Y_test, X_valid, Y_valid]
					return splits
			\end{lstlisting}
			\caption{\emph{source code:} data \emph{training}, data \emph{testing} dan data \emph{validation} 
			disimpan kedalam \textit{array} \texttt{X\_train, Y\_train, X\_test, Y\_test, X\_valid, Y\_valid}. 
			Yang lalu disimpan kedalam \texttt{splits}}
			\label{code: spliting dataset}
		\end{figure}

		Label baru ditambahkan sekarang agar tidak menggangu proses penulisan kolom .csv yang dinamis 
		mengikuti jumlah fitur yang ada. Hasilnya adalah sebuah \textit{object} bernama \texttt{splits} 
		yang memiliki \textit{dataframe}: \texttt{X\_train, Y\_train, X\_test, Y\_test, X\_valid, Y\_valid}. \textit{Dataframe} 
		dengan data awalan X berisikan nilai-nilai fitur yang sudah dikalkulasi di tahap sebelumnya. 
		Sementara data awalan Y berisikan label untuk data X.

		\texttt{split\_data()} mengambil data dari .csv menggunakan fungsi bernama \texttt{get\_data}. Fungsi ini 
		hanya bertugas untuk membaca .csv saja dengan bantuan fungsi \texttt{read\_csv} dari \textit{library} Pandas:
		
		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
				def get_data(features, csv_name):
					col_names = ['image_ids']
					for i in range(len(features)):
						temp_column_name = f'win_1_feature_{i}'
						col_names.append(temp_column_name)
					return Utilities.read_csv(csv_name, col_names)

				def read_csv(csv_name, col_names):
					# used to read all column
					directory = "Data/" + csv_name + ".csv"
					data = pd.read_csv(directory, skiprows=1, header=None, names = col_names)
					return data
			\end{lstlisting}
			\caption{\emph{source code:} get data dan readcsv yang digunakan oleh split data}
			\label{code: get data and read csv}
		\end{figure}

		setelah data di-\textit{split}, barulah \emph{decision tree} bisa deibuat dengan menggunakan 
		fungsi \texttt{build\_all\_tree}:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
				def build_all_tree(splits, features):
					classifiers = [None] * len(features)
					classifiers_accuracy = [0] * len(features)
					X_train, Y_train, X_test, Y_test, X_valid, Y_valid = splits
					minimum_splits = 3
					maximum_depth = 3
					for i in range(len(features)):
						if i % 1000 == 0: print (f'starting tree {i}')
						classifier = DecisionTreeClassifier(minimum_splits, maximum_depth)
						classifier.fit(temp_X_train, Y_train)
						# classifier.print_tree()

						classifiers[i] = classifier
						Y_pred = classifier.predict(X_test)
						classifiers_accuracy[i] = accuracy_score(Y_test, Y_pred)
					return classifiers, classifiers_accuracy
			\end{lstlisting}
			\caption{\emph{source code:} \texttt{build\_all\_tree} untuk membuat semua decision tree untuk setiap fitur}
			\label{code: make all decision tree}
		\end{figure}

		Pada tahap ini \texttt{build\_all\_tree} mengisolasi kolom nilai fitur yang berhubungan 
		dan menyimpannya pada \texttt{temp\_X\_train}, yang nantinya akan digunakan saat pembuatan 
		\emph{decision tree}. \emph{Decision tree} di-inisialisais membuat \textit{class}
		\texttt{DecisionTreeClassifier} dan disimpan menjadi \texttt{classifier}. 
		\texttt{Classifier} lalu dilatih menggunakan fungsi \texttt{fit()}. Hasil dari pelatihan ini lalu langsung dites menggunakan 
		fungsi \texttt{accuracy\_score()} dari \textit{library} Sklearn. Variabel \texttt{classifiers\_accuracy} ini nantinya 
		akan dipakai dalam proses \emph{Boosting}. Untuk keseluruhan \textit{class} \texttt{DecisionTreeClassifier} 
		dan fungsi-fungsinya bisa dilihat berkut ini:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
				class Node():
					def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):
						self.feature_index = feature_index
						self.threshold = threshold
						self.left = left
						self.right = right
						self.info_gain = info_gain

						self.value = value
			\end{lstlisting}
			\caption{\emph{source code: class} \texttt{node} digunakan untuk menyimpan informasi cabang dan
			\textit{threshold} pada \emph{node decision tree}}
			\label{code: node class}
		\end{figure}	

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				class DecisionTreeClassifier():
					def __init__(self, minimum_splits = 2, maximum_depth = 2):
						self.root = None

						# stopping condition
						self.minimum_splits = minimum_splits
						self.maximum_depth = maximum_depth

			\end{lstlisting}
			\caption{\emph{source code: class} digunakan untuk menyimpan tinggi maksimal dan minimal 
			\textit{split} pada \emph{decision tree}. Semua data lainnya disimpan pada \texttt{node}}
			\label{code: DecisionTreeClassifier class}
		\end{figure}

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
						
				def build_tree(self, training_dataset, current_depth = 0):

					X, Y = training_dataset[:,:-1], training_dataset[:,-1]
					num_samples, num_features = np.shape(X)
			
					# split until conditons are met
					if num_samples >= self.minimum_splits and current_depth <= self.maximum_depth:
						# find best split
						best_split = self.get_best_split(training_dataset, num_features)
						# check if information gain is positive
						if best_split["info_gain"]>0:
							left_subtree = self.build_tree(best_split["dataset_left"], current_depth+1)
							right_subtree = self.build_tree(best_split["dataset_right"], current_depth+1)
							return Node(best_split["feature_index"], best_split["threshold"], left_subtree, right_subtree, best_split["info_gain"])
						
					leaf_value = self.calculate_leaf_value(Y)
					return Node(value=leaf_value)
					
			\end{lstlisting}
			\caption{\emph{source code:} fungsi utama dari \emph{class} DecisionTreeClassifier}
			\label{code: build tree function}
		\end{figure}

		Fungsi \texttt{build\_tree()} adalah fungsi utama untuk pelatihan \emph{decision tree} yang berjalan secara 
		rekursif sampai sebuah daun sudah didapat, atau kedalaman maksimum sudah dicapai. 
		Sebuah daun sudah didapat bilamana \texttt{info gain} dari fungsi \texttt{get\_best\_split} adalah 0, 
		atau \textit{node} sudah tidak perlu dipecah lagi karena mencapai potensi maksimum \textit{decision tree}.

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def get_best_split(self, dataset, num_features):
					# dictionary to save data
					best_split = {
					"info_gain": -float("inf")  # Initialize info_gain to a very small value
					} 
					max_info_gain = -float("inf")

					for feature_index in range(num_features):
						feature_values = dataset[:, feature_index]
						potential_thresholds = np.unique(feature_values)

						for threshold in potential_thresholds:
							# get curent split
							dataset_left, dataset_right = self.split(dataset, feature_index, threshold)
							# check if child not null
							if len(dataset_left) > 0 and len(dataset_right) > 0:
								y, left_y, rigth_y = dataset[:,-1], dataset_left[:,-1], dataset_right[:, -1]
								# compute information gain
								current_info_gain = self.information_gain(y, left_y, rigth_y, "gini")
								# update best split if needed
								if current_info_gain > max_info_gain:
									best_split["feature_index"] = feature_index
									best_split["threshold"] = threshold
									best_split["dataset_left"] = dataset_left
									best_split["dataset_right"] = dataset_right
									best_split["info_gain"] = current_info_gain
									max_info_gain = current_info_gain

        			return best_split

			\end{lstlisting}
			\caption{\emph{source code:} fungsi get\_best\_split}
			\label{code: get split function}
		\end{figure}

		Fungsi \texttt{get\_best\_split()} bertugas untuk mencari \textit{threshold} paling sesuai 
		untuk memecah cabang suatu \textit{node} dengan mengetes satu-persatu nilai 
		atribut dari data latihan. Atribut disini adalah nilai \textit{feature} yang sedang dilatih 
		dari semua gambar dari set \textit{train}. Untuk mencari \textit{info gain}, \textit{gini} 
		akan dihitung menggunakan fungsi \texttt{information\_gain}. Selain \textit{gini}, 
		menghitung \texttt{information gain} juga dapat dilakukan dengan menggunakan \textit{entropy}.

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def split(self, dataset, feature_index, threshold):
						# fuction to split data 
						dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])
						dataset_rigth = np.array([row for row in dataset if row[feature_index] > threshold])
						return dataset_left, dataset_rigth
			
			\end{lstlisting}
			\caption{\emph{source code:} fungsi split hanya bertugas membagi \texttt{node} 
			berdasarkan \textit{threshold} yang sudah ditemukan}
			\label{code: split function}
		\end{figure}

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]
					
					def information_gain(self, parent, left_child, right_child, mode="entropy"):
						weight_left = len(left_child) / len(parent)
						weight_rigth = len(right_child) / len(parent)
						if mode == "gini":
							gain = self.gini_index(parent) - (weight_left * self.gini_index(left_child) + weight_rigth * self.gini_index(right_child))
						else:
							gain = self.entropy(parent) - (weight_left * self.entropy(left_child) + weight_rigth * self.entropy(right_child))
						return gain
					
			\end{lstlisting}
			\caption{\emph{source code:} \texttt{information\_gain()} mencari data dengan menghitung \emph{gini} 
			atau \textit{entropy}}
			\label{code: split function}
		\end{figure}
		
		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def entropy(self, y):
						# fuction to count entropy
						class_labels = np.unique(y)
						entropy = 0
						for cls in class_labels:
							p_cls = len(y[y == cls]) / len(y)
							entropy += -p_cls * np.log2(p_cls)
						return entropy
					
			\end{lstlisting}
			\caption{\emph{source code:} perhitungan \emph{entrophy}}
			\label{code: entrophy calculation function}
		\end{figure}

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def gini_index(self, y):
						# function to count gini index (lebih cepet aja karna gak pake log)
						class_labels = np.unique(y)
						gini = 0
						for cls in class_labels:
							p_cls = len(y[y == cls]) / len(y)
							gini += p_cls**2
						return 1 - gini

			\end{lstlisting}
			\caption{\emph{source code:} perhitungan \emph{gini}}
			\label{code: gini calculation function}
		\end{figure}

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def calculate_leaf_value(self, Y):

						Y = list(Y)
						return max(Y, key = Y.count)

			\end{lstlisting}
			\caption{\emph{source code:} fungsi untuk mencari mayoritas 
			kelas pada \textit{leaf node}}
			\label{code: find majority class in node function}
		\end{figure} 

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def fit(self, X, Y):
						# fuction to train tree 
						dataset = np.concatenate((X, Y), axis = 1)
						self.root = self.build_tree(dataset)

			\end{lstlisting}
			\caption{\emph{source code:} fungsi \texttt{fit()} adalah fungsi 
			yang dipanggil untuk mulai membangun \emph{decision tree} setelah dibuat}
			\label{code: fit function}
		\end{figure}

		Terakhir, fungsi \texttt{Predict()} akan digunakan untuk klasifikasi yang sebenarnya. Dalam fugnsi ini 
		fit mengambil X atau \textit{dataset} \texttt{X\_test} untuk mengetes akurasi dari \emph{decision tree} 
		tersebut yang lalu akan dikomparasi dengan \texttt{Y\_test} menggunakan fungsi sklearn \texttt{accurac\_score()}. 
		Pencarian \texttt{accurac\_score()} setiap \emph{decision tree} disini dilakukan untuk 
		mempercepat proses \emph{boosting} di tahap berikutnya karena \emph{decision tree} 
		dapat langsung diurutkan dari yang terkuat ke yang terlemah.

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

					def predict(self, X):
						# fuction to predict new dataset 
						predictions = [self.make_prediction(x, self.root) for x in X]
						return predictions
					
					def make_prediction(self, x, tree):
						# fuction to detect single datapoint
						if tree.value!=None: return tree.value
						feature_val = x[tree.feature_index]
						if feature_val <= tree.threshold: return self.make_prediction(x, tree.left)
						else: return self.make_prediction(x, tree.right)
			\end{lstlisting}
			\caption{\emph{source code:} \texttt{Predict()} digunakan untuk melakukan prediksi 
			dengan \emph{decision tree} yang sudah dibuat}
			\label{code: predict function}
		\end{figure}

		Setelah semua \emph{decision tree} dan akurasinya sudah dicari dan disimpan kedalam 
		\textit{array} classifiers dan classifiers\_accuracy. Keduanya akan disimpan kedalam 
		dokumen Pickle untuk direferensi kedepannya. Penyimpanan kedalam dokumen pickle ini bertujuan 
		agar proses pelatihan tidak perlu diulangi berulang kali bila ada masalah di tahapan berikutnya. 
		Berikut \textit{source code} penyimpanan \emph{decision tree} kedalam Pickle:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				class PickleTree:
					def __init__(self, features, trees, accuracies):
						self.feature_num = np.arange(len(features))
						self.trees = trees
						self.accuracies = accuracies
					
				def dump_to_pickle(file_name, object):
					directory = "Data/" + file_name + ".pickle"
					with open(directory, 'wb') as file:
						pickle.dump(object, file)
			\end{lstlisting}
			\caption{\emph{source code:} penyimpanan \emph{decision tree} kedalam pickle}
			\label{code: save decision tree}
		\end{figure}

		Untuk menyimpan \emph{decision tree} kedalam pickle, pertama semua \emph{decision tree} 
		dan akurasinya disimpan kedalam \textit{class} bernama \texttt{PickleTree} yang lalu akan di \textit{dump} 
		menggunakan fungsi \texttt{dunp\_to\_pickle()} kedalam \textit{directory} yang sudah ditentukan.

	\subsection{\textit{Boosting}}

		Setelah dokumen pickle dari semua \emph{decision tree} atau \emph{weak classifier} 
		dibuat. Semua \emph{weak classifier} akan di-\emph{boosting} untuk memberikan bobot \textit{voting} 
		untuk semuanya. Hal ini dilakukan dengan mengetes \emph{weak classifier} secara berurutan dari 
		yang terkuat ke yang terlemah. Contoh-contoh latihan yang sulit untuk diklasifikasi 
		\emph{weak learner} sebelumnya akan diberikan nilai lebih bila berhasil diklasifikasi 
		\emph{weak learner} berikutnya. Proses ini kita mulai dengan memanggil fungsi 
		\texttt{training\_strong\_classifier()}:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def training_strong_classifier(features, trees, splits, accuracy, pickle_name):
					X_train, Y_train, X_test, Y_test, X_valid, Y_valid = splits
					image_weights = Boosting.initialize_weight(Y_test)
					print(np.sum(image_weights))
					orderlist = np.arange(len(accuracy))
					orderlist = Boosting.get_initial_sorted_accuracy(accuracy, orderlist)

					initial_accuracy = float('-inf')
					current_accuracy = 0
					iteration = 0
					limit = 100 #change according to needs

					# start boosting loop. Will stop when accuracy fell or iteration hit limit
					while True:
						alpha_list = Boosting.start_boosting(trees, X_test, Y_test, image_weights, orderlist)
						validation_prediction = Boosting.strong_prediction(trees, orderlist, X_valid, alpha_list)

						initial_accuracy = current_accuracy
						print(f'current initial accuracy: {initial_accuracy}')
						current_accuracy = accuracy_score(Y_valid, validation_prediction)
						print(f'current after boosting accuracy: {current_accuracy}')
						
						# check whether accruacy deteriorate or limit hit
						if current_accuracy <= initial_accuracy or iteration >= limit:
							print('final accuracy deteriorate, rolling back to last iteration...')
							alpha_list = last_iteration_alpha_list
							orderlist = last_iteration_orderlist
							break
						
						print('starting over. Saving alpha...')
						alpha_list, orderlist = Boosting.get_sorted_accuracy(alpha_list, orderlist)
						last_iteration_alpha_list = alpha_list
						last_iteration_orderlist = orderlist
						iteration += 1


					# saving trees, related features and its order in pickle
					final_trees = np.empty(len(orderlist), dtype=object)
					final_features = np.empty(len(orderlist), dtype=object)
					for i in range(len(orderlist)):
						final_trees[i] = trees[orderlist[i]]
						final_features[i] = features[orderlist[i]]

					pickle_this = PickleTreeFinal(final_features, final_trees, alpha_list)
					Utilities.dump_to_pickle(f'{pickle_name}', pickle_this)
			\end{lstlisting}
			\caption{\emph{source code:} training\_strong\_classifier}
			\label{code: training strong classifier}
		\end{figure}

		Pertama dalam fungsi ini harus dicari bobot nilai dari setiap contoh latihan, 
		bobot nilai ini berbeda dari bobot \emph{voting weak learner}. Fungsi bobot nilai 
		adalah menaikan kuatnya \textit{voting} sebuah \textit{weak classifier} 
		bila \emph{weak learner} berhasil mengklasifikasi sebuah 
		contoh latihan dengan benar, oleh karena itu jumlah nilai total dari bobot 
		latihan atau image\_weights haruslah kurang lebih satu. \texttt{image\_weights} di-inisialisasi 
		menggunakan fungsi \texttt{initialize\_weights()} berikut:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def initialize_weight(test_images):
					image_weights = np.ones(len(test_images)) / len(test_images)
					return image_weights
			\end{lstlisting}
			\caption{\emph{source code:} inisialisasi bobot gambar sebelum /emph{Boosting}}
			\label{code: initialize image weights}
		\end{figure}

		Bisa dilihat proses penghitungan bobot dari \texttt{image\_weights} hanyalah pembagian 
		satu dengan jumlah total contoh latihan (disini len(\texttt{test\_images})). Untuk proses 
		pelatihan menggunakan 80 contoh gambar latihan, fungsi \texttt{split()} telah mengalokasikan 
		28 contoh untuk digunakan dalam tahap \emph{Boosting}, yang disimpan dalam \texttt{X\_valid} 
		dan \texttt{Y\_valid}. Berikutnya fungsi \texttt{get\_initial\_sorted\_accuracy()} dipanggil untuk 
		mengurutkan \emph{weak classifier} berdasarkan akurasi yang sudah didapat pada tahap sebelumnya:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def get_initial_sorted_accuracy(accuracy, orderlist):
					accuracy_threshold = 0.4
					accuracy, orderlist = zip(*sorted(zip(accuracy, orderlist), reverse = True))
					orderlist = [classifier for accuracy, classifier in zip(accuracy, orderlist) if accuracy >= accuracy_threshold]
					return orderlist

			\end{lstlisting}
			\caption{\emph{source code:} pengurutan \emph{weak classifier} berdasarkan akurasi}
			\label{code: initial sorted accyracy}
		\end{figure}

		Pada tahap \textit{sorting} ini juga dilakukan elimanasi \emph{weak classifier} yang terlalu lemah. 
		Awalnya penulis mengeliminasi \emph{weak classifier} yang memiliki nilai akurasi dibawah 50\% 
		namun karena takut jumlah \emph{weak classifier} terlalu sedikit, maka penulis menurunkan \textit{threshold} 
		menjadi 40\%. Eliminasi ini secara signifikan mengurangi jumlah \emph{classifier} yang awalnya berjumlah 
		sekitar 520.000 menjadi: 6742 \emph{weak classifier} pada \emph{classifier} jendela kiri, 
		8231 \emph{weak classifier} pada \emph{classifier} jendela tengah, dan 
		10588 \emph{weak classifier} pada \emph{classifier} jendela kanan. Eliminasi yang besar ini 
		mengimplikasikan bahwa mayoritas \emph{weak classifier} yang dibuat dengan mencoba 
		semua probabilitas yang ada memiliki akurasi dibawah 40\% dan mungkin hanya akan berkontribusi 
		saja kepada klasifikasi akhir. Selanjutnya proses \emph{Boosting dilanjutkan} dengan 
		mencari alpha\_list atau bobot voting setiap \emph{weak classifier} menggunakan fungsi 
		start\_boosting(). Berikut adalah \textit{source code}-nya:

		% ini harus diganti pas model baru kelar

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def start_boosting(trees, X_test, Y_test, image_weights, orderlist):
					print('Boosting...')
					alpha_list = np.zeros(len(orderlist))
					for i in range(len(orderlist)):
						# make prediction with i-th tree
						treeN = orderlist[i]
						prediction  = trees[treeN].predict(X_test)

						# calculate error of the tree
						indicator = np.array(np.array(prediction).astype(int) != Y_test.flatten(), dtype = float)
						epsilon = np.sum(image_weights * indicator) / np.sum(image_weights)

						# calculate the weight of the tree
						alpha = 0.5 * np.log((1 - epsilon) / (epsilon + 1e-10)) + np.log(4 - 1) #1e-10 const added to prevent div by 0. 4 is number of class
						if alpha < 1e-10: alpha = 1e-10 #1e-10 const added to prevent alpha getting too small in np.exp(alpha * indicator) later
						alpha_list[i] = alpha

						# update the weight for the samples so the sum of image_weight will be close to 1 for the next iteration
						image_weights *= np.exp(alpha * indicator)
						image_weights /= np.sum(image_weights)

					return alpha_list

			\end{lstlisting}
			\caption{\emph{source code:} pencarian nilai bobot \emph{boosting} menggunakan fungsi 
			\texttt{start\_boosting()}}
			\label{code: start boosting}
		\end{figure}

		Disini hasil klasifikasi yang dilakukan oleh \emph{weak classifier} akan dibandingkan 
		dengan label aslinya yang tersimpan di \texttt{Y\_valid} dan disimpan pada array \texttt{indicator} 
		dalam nilai 0 bila klasifikasi dilakukan secara benar, dan 1 bila klasifikasi dilakukan 
		secara salah. Kemudian \texttt{alpha} atau bobot voting sang \emph{weak classifier} 
		akan dihitung. Para perhitungan ini, \texttt{epsilon} akan ditambahkan dengan 1e-10 
		untuk mencegah pembagian dengan angka 0 bilamana \emph{weak classifier} benar mengklasifikasi 
		semua contoh dan menghasilnya \texttt{indicator} yang hanya berisi angka 0 saja. np.log(4 - 1) 
		disini digunakan agar \texttt{alpha} tidak negatif, 4 pada formula ini adalah jumlah kelas 
		yang sedang diklasifikasi yaitu kelas negatif, Abudefduf, Amphiprion, dan Chaetodon. Berikutnya 
		\texttt{alpha} dicek supaya tidak lebih kecil daripada 1e-10 agar tidak menyebabkan normalisasi 
		bobot gambar yang salah di bagian berikutnya. terakhir \texttt{image\_weight} diupdate, dimana gambar 
		yang salah diklasifikasi akan dinaikan nilainya, baru setelahnya nilai dinormalisasi lagi 
		agar kurang lebih berjumlah 1. Setelah bobot \textit{voting} sudah dicari, seluruh 
		\emph{weak learner} pada tahap ini akan dites layaknya klasifikasi yang sebeneranya, dimana 
		nilai \textit{voting} setiap \emph{weak classifier} akan diperhitungkan untuk memilih 
		hasil klasifikasi. Klasifikasi pada tahap ini dilakukan oleh fungsi \texttt{strong\_prediciton}:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def strong_prediction(trees, orderlist, X_valid, alpha_list):
					predictions = [0] * len(X_valid)
					scoreboard = [[0, 0, 0, 0] for _ in range(len(X_valid))]
					for i in range(len(orderlist)):
						tree_index = orderlist[i]
						prediction = trees[tree_index].predict(X_valid)

						# add score to scoreboard according to results and alpha value of tree
						for j in range(len(prediction)):
							weak_learner_prediction = int(prediction[j])
							scoreboard[j][weak_learner_prediction] += 1 * alpha_list[i]
					
					# return score to the main scoreboard
					for k in range(len(prediction)):
							# print(f'scoreboard {k}: {scoreboard[k]}')
							predictions[k] = scoreboard[k].index(max(scoreboard[k]))
					return predictions

			\end{lstlisting}
			\caption{\emph{source code:} klasifikasi yang dilakukan setelah setiap iterasi \emph{boosting}}
			\label{code: strong prediction}
		\end{figure}

		Saat klasifikasi. akan dibuatkan variabel \texttt{scoreboard} untuk mencatat total nilai voting 
		setiap kelas. Contohnya suatu \emph{weak classifier} dengan bobot \textit{voting} 0.67 
		mengklasifkasi suatu fitur sebagai kelas 1 atau Abudefduf. maka \emph{scoreboard} akan 
		berubah manjadi [0, 0.67, 0, 0]. Lalu misalnya \emph{weak classifier} lain dengan bobot 
		\textit{voting} 0.2 memilih kelas 3 atau Amphiprion, maka \emph{scoreboard} akan menjadi 
		[0, 0.67, 0.2, 0]. Klasifikasi akan diakhiri ketika semua \emph{weak learner} sudah 
		dipakai. Setelah itu kelas dengan nilai \textit{voting} paling tinggi akan dipilih sebagai 
		hasil dari klasifikasi. Yang lalu akan dibandingkan dengan \texttt{Y\_valid} untuk dicari akurasinya.

		Proses iterasi \emph{boositng} ini akan diulang terus menerus hingga tingkat akurasi klasfikasi 
		menggunakan \emph{weak classifier} berbobot mulai mengalami penurunan. Dalam situasi ini 
		nilai bobot \textit{voting} dan urutan \textit{voting} pada iterasi sebelumnya akan diambil dan disimpan 
		kedalam dokumen pickle, kali ini dalam \textit{class} \texttt{PickleTreeFinal} dengan fungsi 
		\texttt{dump\_to\_pickle()} sebelumnya:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				class PickleTreeFinal:
					def __init__(self, features, trees, alpha_list):
						self.features = features
						self.trees = trees
						self.alpha_list = alpha_list

			\end{lstlisting}
			\caption{\emph{source code:} bentuk \textit{class PickleTreeFinal}}
			\label{code: PickleTreeFinal class}
		\end{figure}

		Perbedaan pickle ini dengan pickle yang menyimpan seluruh fungsi pada tahap sebelumnya 
		adalah pada pickle ini juga disimpan info \emph{feature} juga sesuai dengan urutan dari 
		\emph{weak classifier} yang berhubungan. Hal ini dilakukan agar pada tahapan berikutnya, 
		\emph{cascade} dapat langsung digunakan untuk klasifikasi sebenarnya, yang memerlukan info 
		\emph{features} untuk dapat langsung membaca nilai fitur langsung dari gambar. Hasil dari \emph{boosting} untuk ketiga \emph{strong classifier} cukup memuaskan 
		dengan akurasi 71\% untuk ketiga \emph{strong classifier}.
		
		\begin{table}[h]
			\caption{Hasil boosting pada ketiga \textit{window}}
			\label{tab:mytable}
			\centering
			\begin{tabular}{|c|p{2cm}|c|p{3cm}|}
				\hline
				\textbf{Window pickle name} & \textbf{Num. Features} & \textbf{Accuracy} & \textbf{Top Feature} \\
				\hline
				window\_0\_strong\_classsifier & 2751 & 71\% & ('Right Triangular', 26, 24, 4, 4) \\
				\hline
				window\_1\_strong\_classsifier & 4521 & 71\% & ('Four Diagonal', 4, 40, 8, 4) \\
				\hline
				window\_2\_strong\_classsifier & 6749 & 71\% &  ('Two Horizontal', 0, 0, 4, 4) \\
				\hline

			\end{tabular}
		\end{table}
		
	\subsection{\textit{Training Cascade}}
		
		Pelatihan pickle dimulai dengan pertama membuat \textit{class} \texttt{Cascade} yang 
		nantinya akan diisi dengan \textit{class} \texttt{stage} yang berisikan \emph{weak classsifier} 
		dengan bobot voting mereka. Berikut bentuk \textit{class} Cascade:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				class Cascade:

					def __init__(self):
						self.stages = []

			\end{lstlisting}
			\caption{\emph{source code:} bentuk \textit{class Cascade}}
			\label{code: Cascade class}
		\end{figure}

		Selanjutnya \texttt{stages} akan diisi dengan fungsi \texttt{fill\_cascade()} dengan 
		\textit{source code} sebagai berikut:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def fill_cascade(self, features, trees, alpha_list, splits):
					print(f'starting to fill cascade...')
					X_train, Y_train, X_test, Y_test, X_valid, Y_valid = splits
					used_features = 0
					print(f'number of used_features: {used_features}')
					while True:
						if used_features >= len(features): break
						new_cascade = CascadeStage()
						new_cascade.train_stage(features, trees, alpha_list, X_valid, Y_valid, used_features)
						used_features += len(new_cascade.trees) #check the total number of features used
						self.stages.append(new_cascade)
					print(f'cascade is finished!')

			\end{lstlisting}
			\caption{\emph{source code:} fungsi untuk mengisi \texttt{stages} pada 
			\emph{Cascade}}
			\label{code: fill cascade function}
		\end{figure}

		Fungsi \texttt{fill\_cascade} disini mengambil \texttt{features, trees} dan \texttt{alpha\_list} 
		yang sudah diurutkan dan di-\emph{boosting} pada tahap sebelumya. Lalu sebuah \textit{class} 
		baru dibuat untuk menyimpan ketiganya pada sebuah \emph{stage}, \textit{class} itu adalah 
		\texttt{CascadeStage} dengan \textit{source code} sebagai berikut:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				class CascadeStage:
					def __init__(self):
						self.features = []
						self.trees = []
						self.alpha_list = []

			\end{lstlisting}
			\caption{\emph{source code: class} CascadeStage}
			\label{code: Cascade Class}
		\end{figure}

		yang lalu diisi dengan menggunakan fungsi \texttt{train\_stage()} sebagai berikut:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def train_stage(self, features, trees, alpha_list, X_valid, Y_valid, used_features):
					detection_rate = 0
					while detection_rate < 0.5:
						if used_features >= len(features): break
						# append weak classifier into stage one by one
						self.features.append(features[used_features])
						self.trees.append(trees[used_features])
						self.alpha_list.append(alpha_list[used_features])

						orderlist = np.arange(len(self.trees))
						validation_prediction = Boosting.strong_prediction(self.trees, orderlist, X_valid, self.alpha_list)
						detection_rate = accuracy_score(Y_valid, validation_prediction)
						used_features += 1
					print(f'features used in this stage: {used_features}')

			\end{lstlisting}
			\caption{\emph{source code: } pelatihan \emph{stage} dalam \emph{Cascade}}
			\label{code: train stage function}
		\end{figure}

		Pada fungsi ini \emph{weak classifier} ditambah satu persatu berserta bobot dan 
		\emph{feature}-nya, lalu \emph{stage} akan dites menggunakan fungsi \texttt{strong\_prediction} 
		yang dipakai juga di tahap \emph{boosting}. Penggunaan fungsi ini dapat dilakukan 
		karena struktur dan cara klasifikasi dari \emph{stage} mirip dengan sederet \emph{weak classifier} 
		pada tahap sebelumnya. Iterasi ini lalu diteruskan hingga akurasi \emph{stage} mencapai 50\% 
		atau sampai \emph{weak classifier} habis. Setelah itu \emph{stage} yang sudah dibuat 
		akan di-\texttt{append()} kedalam \textit{array stages} dan proses yang sama diulangi hingga 
		semua \emph{weak classifer} sudah habis terpakai. Terakhir \emph{Cascade} lalu disimpan 
		kedalam pickle untuk digunakan dalam klasifikasi yang sebenarnya. Berikut fungsi yang 
		digunakan untuk menyimpan \emph{cascade} kedalam pickle:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				def save_to_pickle(self, pickle_name):
					print(f'saving to Pickle...')
					Utilities.dump_to_pickle(f'{pickle_name}', self)
					print(f'complete!')

			\end{lstlisting}
			\caption{\emph{source code: } fungsi penyimpanan \emph{Cascade} ke pickle 
			dengan menggunakan fungsi \texttt{dump\_to\_pickle} lagi}
			\label{code: save Cascade to pickle}
		\end{figure}

		\begin{figure}[H]
			\centering{}
			  \includegraphics[width=0.8\textwidth]{gambar/flowchart\_cascade}
			\caption{Gambaran \textit{cascade} window 1 (kiri) dan cara kerjanya}
		\end{figure}

\section{Validasi}

		Untuk proses validasi atau penggunaan, penulis telah membuat sebuah \textit{file} 
		python baru untuk mengklasifikasi menggunakan \emph{cascade} yang telah dibuat. Untuk gambar 
		yang akan diklasifikasi harus memiliki ukuran 350 x 200 piksel, bertipekan \textit{Portable Network Graphics} 
		atau PNG, dan dengan latar belakang sudah dihilangkan. Gambar-gambar yang akan diklasifikasi harus 
		dimasukan kedalam \textit{folder} bernama \emph{classification\_target}, dan pengguna juga 
		membuat satu \textit{folder} lain bernama \emph{classification\_results} untuk hasil klasifikasi. 
		Totalnya penulis telah mengumpulkan 75 gambar ikan baru dengan jumlah tiap kelas 25 gambar.
		Berikut \textit{source code} dari predict.py:

		\begin{figure}[H]
			\begin{lstlisting}[language=Python, basicstyle=\tiny]

				import numpy as np
					import os
					import cv2
					from Cascade import *
					from Utilities import *

					# load cascades for each window
					window_cascade = [None, None, None]
					window_prediction = np.zeros(3)
					window_cascade[0] = Utilities.read_from_pickle('window_0_cascade') #for left side/mouth detection
					window_cascade[1] = Utilities.read_from_pickle('window_1_cascade') #for mid side/fin detection
					window_cascade[2] = Utilities.read_from_pickle('window_2_cascade') #for right side/tail detection

					directory = "classification_target"

					for filename in os.listdir(directory):
						if filename.endswith(".png"):
							image_path = os.path.join(directory, filename)
							image_name = filename

							#load target image for classification
							image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
							image_unedited = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)
							image = cv2.resize(image, (350, 200))
							image_width = 350
							image_height = 200

							# scan for the whole image using sliding windows
							for i in range(len(window_cascade)):
								# offset for different part all 3 window 
								match i:
									case 0: left_window_width = 0
									case 1: left_window_width = int(image_width / 3)
									case 2: left_window_width = int(image_width / 3 * 2)
								for y in range(0, image_height - 50 + 1):
									for x in range(0, int(image_width / 3) - 50 +1):
										prediction = window_cascade[0].final_cascade_classification(image, x + left_window_width, y)
										if prediction != 0: break
									
									if prediction != 0: break
								# print(f'classification result for window {i}: {prediction}')
								window_prediction[i] = prediction

							# count majority vote and predict class
							print(f'result of {image_name} classification: {window_prediction}')
							unique_elements, counts = np.unique(window_prediction, return_counts=True)
							max_count_index = np.argmax(counts)

							if counts[max_count_index] > len(window_prediction) // 2:
								image_class = unique_elements[max_count_index]
							else:
								image_class = 0

							match image_class:
								case 0: image_class = 'None'
								case 1: image_class = 'Abudefduf'
								case 2: image_class = 'Amphiprion'
								case 3: image_class = 'Chaetodon'

							position = (10, 30)
							font = cv2.FONT_HERSHEY_SIMPLEX
							font_scale = 1
							font_thickness = 2
							font_color = (0, 0, 255)

							output_image_path = os.path.join('classification_results\\', os.path.splitext(filename)[0] + '.jpg')
							cv2.putText(image_unedited, image_class, position, font, font_scale, font_color, font_thickness)
							cv2.imwrite(output_image_path, image_unedited)
							print('anotated image completed!')

			\end{lstlisting}
			\caption{\emph{source code: } predict.py untuk melakukan klasifikasi sebenarnya}
			\label{code: predict.py}
		\end{figure}

		\begin{figure}[H]
			\centering{}
			  \includegraphics[width=0.4\textwidth]{gambar/fish\_classes}
			\caption{Ketiga kelas ikan}
		\end{figure}

		\texttt{Predict.py} pertama membaca gambar-gambar yang akan diklasifikasi dari folder 
		\textit{classification\_target}, metode yang digunakan kurang lebih sama dengan 
		yang digunakan pada \texttt{load\_image()}. Lalu program akan melakukan \textit{looping} 
		dari kiri atas gambar, bergerak ke kiri bawah gambar, hal ini adalah \emph{sliding window}. 
		Pada setiap \emph{window}, program akan memanggil cascade yang berhubungan untuk melakukan klasfikasi. 
		Bila \emph{cascade} mengembalikan suatu kelas ikan, maka \textit{looping} untuk area tersebut 
		akan dihentikan dan dilanjutkan ke area berikutnya. Proses \textit{looping} ini dijalankan 
		tiga kali untuk area kiri, tengah dan kanan. Estimasi area kiri adalah x = 0 sampai x = 115 - 50, 
		estimasi area tengah adalah x = 116 sampai x = 232 - 50, dan area kanan x = 233 sampai x = 350 -50. 
		Semua loop dikurangi dengan 50, ukuran dari \emph{sub-window}, agar \textit{slidin window} tidak mencoba mengklasifikasi keluar dari 
		areanya atau keluar dari gambar. Terakhir hasil prediksi akan ditentukan oleh suara 
		mayoritas dari ketiga \textit{window}, kecuali kalau ketiga \emph{window} mem-\textit{voting} 
		kelas yang berbeda, maka kelas 0 atau None akan dipilih. Gambar lalu akan dianotasi dengan nama 
		kelas ditulis di kiri atas gambar \textit{output}, kelas dan informasi penting lainnya juga ditambahkan 
		ke nama dokumen gambar \textit{output} agar mudah diperiksa.

\section{Analisa Hasil}
		Setelah prediksi menggunakan \textit{sliding window} dilakukan, ditemukan bahwa hanya 
		12 dari 75 gambar yang berhasil diklasifikasikan dengan benar. Hasil ini jelas bertolak 
		belangan dengan akurasi yang ditunjukan pada tahap \emph{boosting} dimana ketiga \emph{strong classifier} 
		memiliki skor akurasi diatas 70\%.

		Klasifikasi ikan menggunakan metode \emph{sliding window} menunjukan sebuah problem baru 
		yaitu salahnya prediksi pada lokasi \textit{sub-window} yang tidak seharusnya. \emph{Sliding window} 
		memulai klasifikasi dari pojok kiri atas area klasifikasi dan bergerak kebawah menuju pojok kiri bawah, dengan 
		lokasi akhir klasifikasi pojok kanan bawah. Dalam pergerakannya ini ia akan menolak mayoritas \textit{sub-window} 
		yang tidak memiliki nilai, atau \textit{background}, namun ketika ia menemukan sedikit saja piksel dalam 
		\textit{sub-window}-nya ia akan melakukan klasifikasi. 
		
		\begin{figure}[H]
			\centering{}
			  \includegraphics[width=0.4\textwidth]{gambar/result\_analysis\_2}
			\caption{Gambar tes Amphiprion23. Kotak merah menunjukan ekspektasi klasifikasi 
			yang benar untuk kelas Amphiprion, karena \textit{offset} diletakan disitu. Kotak 
			hijau menunjukan lokasi klasifikasi yang dilakukan \emph{sliding window} ketika mengklasifikasi 
			gambar Amphiprion23}
		\end{figure}
		
		Problem baru lainnya juga keluar pada saat klasifikasi, adalah 
		bias \emph{weak classifier} yang lebih kuat pada awal \emph{cascade} terhadap beberapa kelas tertentu, dalam 
		banyak kasus bias-nya adalah ke kelas ikan Abudefduf. Umumnya \emph{weak classifier} yang paling awal 
		didalam \emph{cascade} memiliki bobot \textit{voting} yang terlalu kuat, sehingga ketika mereka mem-\textit{voting} 
		suatu kelas jumlah total bobot \textit{voting weak classifier} setelahnya tidak dapat mengalahkan 
		\textit{voting weak classifier} paling awal. Dari 50 gambar yang bukan kelas Abudefdu, Abudeduf keluar sebanyak 
		32 kali dalam klasifikasi.
		
		

% Baris ini digunakan untuk membantu dalam melakukan sitasi
% Karena diapit dengan comment, maka baris ini akan diabaikan
% oleh compiler LaTeX.
\begin{comment}
\bibliography{daftar-pustaka}
\end{comment}
